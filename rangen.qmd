# Generating random numbers

*"Anyone who considers arithmetical methods of producing random digits is, of course,
in a state of sin."* (John von Neumann, 1949)

I know, this is abused---you will find this very sentence quoted in any sensible
resource about random numbers you might come across. But, on a second thought,
it *really* encapsulates well the subject we are covering in this section:
generating random sequences by means of arithmetical methods. Quite an oxymoron, eh?


## Prelude: the wheel of fortune

If you were to ask an ordinary person on the street how they would go about drawing
a random sequence, chances are that some sort of variant of the wheel of fortune
would come up pretty high in the list of the most popular answers---provided that
the person would care answering, of course. (I am sure you are familiar with at
least one of the many popular TV shows that are based on the very concept of a
wheel of fortune, but the
[Wikipedia entry](https://en.wikipedia.org/wiki/Wheel_of_Fortune_(medieval))
on the subject is worth at least a quick read, because the idea seems very
deeply rooted, and amusingly so, in western philosophy.)

Now, let us stop for a second and ask ourselves a very fundamental question:
what is the magic in the wheel of fortune that makes its output so unpredictable?
And I am going to stop for a second and let you think about it.

::: {#fig-wheel}
![](figures/wheel.png){width=60%}

A wheel with 10 sectors.
:::

You got it! It is the fact that *the wheel wraps around at every turn*. Let's try
and make this more quantitative with a toy model. For one thing we shall assume
that the wheel has 10 segments, labeled with the integers from 0 to 9, that is,
we are generating one decimal digit. If we say that $\omega_0$ is the initial
angular velocity that we spin the thing with, and $\alpha$ the (modulo of) the
angular deceleration, which for simplicity we shall assume constant in time,
it is easy to show that the wheel stops when the angle $\theta$ has changed by
$$
\theta_0 = \frac{\omega_0^2}{2\alpha}.
$$
If you have an uncertainty $\sigma_\omega$ on $\omega_0$ (which you always do),
you will have an uncertainty of the final angular coordinate of
$$
\frac{\sigma_\theta}{\theta_0} = 2 \frac{\sigma_\omega}{\omega_0}
\quad\text{or}\quad
\sigma_\theta = \frac{2\theta_0}{\omega_0} \sigma_\omega =
\frac{\omega_0}{\alpha} \sigma_\omega
$$
That is: if you have a 1% uncertainty on $\omega_0$, you end up with a 2% uncertainty
on the final value of $\theta$. And this is where things get interesting. If $\theta$
was measuring a distance in a given direction and we were talking about a
bocce-ball court (or a curling court, for what it's worth) our model would describe
a very poor device for generating random sequences: a good player can train to throw
a ball (or a rock) very consistently (i.e., with a small error on the initial
velocity) which means we can reliably predict where the ball itself will end up.
Isn't that the whole point of bocce, by the way?

But if we now go back to our wheel, where $\theta$ wraps up at each turn, *and we
assume that $\omega_0$ is large enough that the things does many turns*, the
situation is completely different. You might have noticed that the absolute error
on $\theta_0$ is proportional to $\omega_0$ and, for any value of $\alpha$ and
$\sigma_\omega$ there will be a value of $\omega_0$ that is large enough for the
error on $\theta$ to be
$$
\sigma_\theta \geq 2\pi.
$$
When that happens, we have lost any capability to predict where the wheel will
land. We might be able to predict roughly *how many turns* the wheel will do, but
not the actual outcome. (In practice, for this to work, you might have to fine-tune
$\alpha$ based on some reasonable assumptions for $\omega_0$ and $\sigma_\omega$,
but this is a technical detail.)

```{python}
import numpy as np

def spin_wheel(omega0, alpha = 0.1, num_segments = 10):
    """Spin our toy wheel of fortune!
    """
    # Calculate the overall angular advance until the wheel stops...
    theta = omega0**2. / 2. / alpha
    # ... wrap around at every turn, i.e., take the modulo 2 pi...
    theta = theta % (2. * np.pi)
    # ... and, finally, calculate the outcome by dividing by the angular with
    # of a sector and taking the integer part.
    return int(theta / (2. * np.pi / num_segments))

# Spin the wheel three times, changing the initial angular velocity by a small
# amount (about 1%), and see that the output is basically random :-)
print(spin_wheel(10.1))
print(spin_wheel(10.2))
print(spin_wheel(10.3))
```

If you got this far, you might be wondering: yeah, this is fun, but how about all
this hype on the wheel of fortune? Weren't we supposed to talk about random numbers?
Well, as it turns out the fortune wheel is a surprisingly good mechanical analogy
to how a typical pseudo-random (ops: did I just say "pseudo"?) number generator works.
The mathematical equivalent of the concept of "wrapping around at every turn" is the
*modulo* operation in ordinary integer arithmetic and it is interesting to note that,
as we shall see in a moment, the modulo is a fundamental ingredient in all the generators
that are relevant for everyday use.
We shall briefly come back to modular arithmetic in section @sec-modular-arithmetic
but, before we do that, there's a few thing we should sort out. In the meantime,
@lecuyer17 provides an engaging and accessible account of the fascinating history
of random number generators.


## Random (and pseudo-random) sequences

While *random numbers* is an expression that you often find in the literature,
you might have noticed that we prefer using *random sequence*. The reason is quite
simple: there is no such a thing as a random number. "Here is a 5---is that random?"
Clearly such a question makes little or no sense. What makes sense, instead,
is the concept of a random sequence. That one we like.

So let's say we have a sequence $X_n$ of integers with $0 \leq X_n < m$. (In our
toy wheel-of-fortune setup, e.g., we had $m = 10$, but we anticipate that, for all
practical purposes, we shall be interested in much larger values for $m$.)
To be concrete, let's say that we spin our wheel of fortune and get
$$
X_n = 2, 5, 1, 3, 9, \ldots
$$
Can we confidently say that this is a random sequence? As it turns out, this is
a *very* hard question that has no good answer. We shall briefly come back to the
issue in @sec-randomness-tests, but we anticipate that, while there are obvious
properties that we would like from a truly random sequence (such as the fact that
all the possible elements appear with the same frequency, within the expected statistical
fluctuations), there is no easy direct *proof* that a given sequence is random.
Usually people apply an entire battery of standard tests and declare that a given
generator is random if the sequences it produces pass all the tests.

It is also worth mentioning that, while the sequence
$$
X_n = 0, 1, 2, 3, 4, 5, \ldots
$$
definitely does not appear to be random (although all the digits are equi-distributed)
in a truly random setting it is just as likely to appear as any other---including
the first one we have shown. You get it: assessing randomness is difficult.

Let us turn for a second to a slightly different question: how could I conceivably
generate a random sequence? Well, arguably I could use a truly random natural
process. Although, when you think really hard about this, it is far from trivial
to devise a truly random process, True Random Number Generators (TRNGs) do exist,
and serve very precise purposes. Have a look at [www.random.org](https://www.random.org)
for a good example, and a good source of information on the topic. What we are
mainly concerned here, though, is how to programmatically generate deterministic
sequences by means of arithmetical methods that behave *as if they were random*.
Such generators are called Pseudo-Random Number Generators (PRNGs) and are the
main topic of this chapter.

The distinction between TRNG and PRNG is a fundamental one, and its importance
can hardly be overstated. The former are by definition aperiodic and non deterministic
(i.e., you cannot repeat the same sequence twice) and tend not to be very efficient.
Yet, if you want to run a lottery, you should probably have a look at this article
about the
[the Michael Larsen incident](https://web.archive.org/web/20170712041017/http://www.rotten.com/library/conspiracy/press-your-luck/)
(amusing reading, I promise) before you consider opting away from a TRNG.

PRNGs, on the other hand, are typically *very* efficient and have the advantage
that you can regenerate a given sequence as many times as you want. This is
definitely your first choice for a Physics simulation---really, it is a no-brainer.


## Tables

There has been a time when tables (and even huge ones) of (true) random digits were
actually a thing. The most famous instance of such endeavors is probably the book
[A Million Random Digits with 100,000 Normal Deviates](https://www.rand.org/pubs/monograph_reports/MR1418.html#top)
published in 1955 (and re-issued in 2001) by the RAND corporation.
If you have 70 euros to spare you can buy one of the last paper copies available online.
(The reviews of the book are quite funny, too.)

Nuisance to use. Memory was precious in the old days. Both motivated the development of
alternative means for producing random numbers.


## Digression: modular arithmetic {#sec-modular-arithmetic}



## Pseudo-random sequences via recurrence

If you happen to be under impression that pseudo-random number generation is a
solved problem, you couldn't be more wrong. For many years linear congruential
generators (LCG) have been the de-facto standard. In 1997 @matsumoto98 caught the
entire World by surprise when they published the Mersenne Twister (MT) algorithm:
with a stunning period of $2^{19937} âˆ’1$ and many other desirable properties it
immediately underwent widespread adoption, and still is the default engine in
many programming languages (including Python) and statistical packages. It took
several years for some definite shortcomings and limitations now widely
recognized to emerge.
In 2014 @oneill2014 published the idea of a family of permuted congruential
generators (PCG), and this is a notable case of an idea that has never been published
in a formal scientific paper, has been harshly criticized in a number of blogs
and other informal forums on the web (see, e.g. [here](https://pcg.di.unimi.it/pcg.php)),
and yet had real-world impact, as PCG is the default random generator in numpy
(and the [related discussion](https://github.com/numpy/numpy/issues/13635) is a
good testament as to how difficult is agreeing on what *the* best PRNG is).

The simplest way to generate a pseudo-random sequence is a by a recurrence
relation of the type
$$
X_{n+1} = f(X_n) \mod m
$${#eq-rnd_recurrence}
where $X_{n+1}$ and $X_n$ are non-negative integers, $m$ is called *modulo*, and
$f(x):~\mathbb{N} \rightarrow \mathbb{N}$ is a suitable function. (More on this
later.) The idea is that you start from a *seed* $0 \leq X_0 < m$ and let all the
other numbers flow from @eq-rnd_recurrence.

Although more complex schemes can (and have been) devised, this very basic
setting is a good one to discuss some of the general properties that are common
to all pseudo-random sequences. The most obvious one is perhaps the fact that
all the numbers in the sequence satisfy $0 \leq X_n < m$, which is to say that
at most $m$ distinct values can be produced. It follows that, sooner or later,
we are bound to stumble across a number that we have already visited, and from
that point on we just go on in circle. We can phrase this very thing in a slightly
different way by saying that, ultimately, *any pseudo-random sequence is periodic*.
In our particular case the period $p$ satisfies $p \leq m$. This is not necessarily
an issue if $p$ is large enough, but definitely important to keep in mind.

At the very basic level, some of the good properties that a PRNG should have
include

* a long period;
* high speed (i.e., $f(x)$ should be easy to calculate on a computer);
* a small memory footprint;
* good statistical properties (for one thing we will want that any reasonably
  long subsequence is equi-distributed in the $[0,~m]$ interval, but as we shall
  see in a moment there are many other desirable properties).

We emphasize that, while having a long period is important, it is not by any means
the *only* important thing. The sequence
$$
X_{n+1} = X_n + 1 \mod m \quad \text{e.g.} \quad 0,~1,~2\ldots m
$$
provides the maximum period $m$, but it is obviously not random.


Before we move on, you might already be uncomfortable with the fact that we are
dealing with integers. Most of our simulations live in the realm of floating-point
numbers, and when do we get to that? Well, you just do
$$
u_n = X_n / m
$$
and here is you sequence of integers turned into a sequence of floating-point
numbers between $0$ and $1$. (It goes without saying, these are by all means
plain, rational numbers, but this is as far as floating-point arithmetics goes
on a digital computer, and we already knew that.) Appendix ??? provides a cursory
summary of floating point arithmetics; in the next chapter we shall discuss how
we turn an equi-distributed sequence in the $[0,~1]$ interval into more interesting
things.


## Middle square

Devised by John Von Neumann at the turn of the 1940s, the middle-square method
if often regarded as the oldest practical method to produce pseudo-random sequences,
as well as an archetypal examples of all the idiosyncrasies that a good generator
should stay away from.

Look at this [blog post](http://bit-player.org/2022/the-middle-of-the-square/).


::: {#fig-middle_square}
![](figures/middle_square.png){width=100%}

Full directed graph of all the possible sequence for a 2-decimal digits middle-square
generator.
:::

## Linear congruential generators


[paper](https://archive.org/details/proceedings_of_a_second_symposium_on_large-scale_/page/140/mode/2up)


## Other generators

Include a few words about crypto generators, and mention the relevant Python
module.


## Tests for randomness {#sec-randomness-tests}


## Survey of the current status

Table of what is used in several